{"cells":[{"cell_type":"markdown","metadata":{"id":"IRZAusNkkjD6"},"source":["**라이브러리 임포트**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K83thxLqqJ61"},"outputs":[],"source":["import os\n","import json\n","import random\n","import shutil\n","import cv2\n","import math\n","import copy\n","\n","from google.colab import drive\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.init as init\n","import torch.optim as optim\n","from torchvision.datasets import ImageFolder\n","from torchvision import transforms, models, datasets\n","from torch.utils.data import DataLoader, Subset"]},{"cell_type":"markdown","metadata":{"id":"419v39A8GQ-A"},"source":["**데이터셋 unzip**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22742,"status":"ok","timestamp":1735187703918,"user":{"displayName":"정서영","userId":"00670050857396282545"},"user_tz":-540},"id":"T_46dgIdpSf4","outputId":"c47ee613-c74e-4504-ab93-b960cd3728d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":201295,"status":"ok","timestamp":1731672852929,"user":{"displayName":"정서영","userId":"11814773768061491291"},"user_tz":-540},"id":"XjLJ0hM1qB0H","outputId":"437f43f0-9064-4c77-f8a6-e28803254424"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/CAB/CAB_dataset/JPEGImages\n"]}],"source":["%cd /content/drive/MyDrive/CAB/CAB_dataset/JPEGImages\n","!unzip -qq \"/content/drive/MyDrive/CAB/CAB_dataset/JPEGImages/images_bbox.zip\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":152358,"status":"ok","timestamp":1731673005280,"user":{"displayName":"정서영","userId":"11814773768061491291"},"user_tz":-540},"id":"NgqLPUCVqDXJ","outputId":"9e607d12-e49c-4d8a-a2ec-8ef30e0b8dec"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/CAB/CAB_dataset/Annotations\n"]}],"source":["%cd /content/drive/MyDrive/CAB/CAB_dataset/Annotations\n","!unzip -qq \"/content/drive/MyDrive/CAB/CAB_dataset/Annotations/label_bbox.zip\""]},{"cell_type":"markdown","metadata":{"id":"ZWMVFXT3ahXm"},"source":["**시드 고정**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1735187703918,"user":{"displayName":"정서영","userId":"00670050857396282545"},"user_tz":-540},"id":"9LCZu01EajNf","outputId":"db6c8d28-0956-43e5-99cf-1b9f7986979d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7c432dd437b0>"]},"metadata":{},"execution_count":3}],"source":["torch.manual_seed(42)"]},{"cell_type":"markdown","metadata":{"id":"THY1Br4gq-8P"},"source":["**데이터 라벨링**"]},{"cell_type":"markdown","metadata":{"id":"aiBVR9Oou_Uu"},"source":["파일 경로 변환\n","- /content/drive/MyDrive/CAB/CAB_dataset/JPEGImages 하위 디렉토리에 있는 이미지를 /content/drive/MyDrive/CAB/CAB_dataset/JPEGImages로 옮긴다.\n","- /content/drive/MyDrive/CAB/CAB_dataset/Annotations 하위 디렉토리에 있는 이미지를 /content/drive/MyDrive/CAB/CAB_dataset/Annotations로 옮긴다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ivkR7gKSuugA"},"outputs":[],"source":["def move_file(base_dir):\n","  for root, dirs, files in os.walk(base_dir):\n","    for file in files:\n","        if file.lower().endswith(('.jpg', 'json')):\n","            source_path = os.path.join(root, file)\n","            destination_path = os.path.join(base_dir, file)\n","\n","            shutil.move(source_path, destination_path)\n","\n","jpeg_dir = \"/content/drive/MyDrive/CAB/CAB_dataset/JPEGImages\"\n","annot_dir = \"/content/drive/MyDrive/CAB/CAB_dataset/Annotations\"\n","\n","move_file(jpeg_dir)\n","move_file(annot_dir)"]},{"cell_type":"markdown","metadata":{"id":"zEP_Pgl0kvel"},"source":["입과 눈 사진을 저장할 폴더를 만들어 준다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sDL7ofFGvBIM"},"outputs":[],"source":["data_root = \"/content/drive/MyDrive/CAB/CAB_dataset/\"\n","eyes_mouth = os.path.join(data_root, \"EyesMouth\")\n","os.makedirs(eyes_mouth, exist_ok=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2_RDzMQPygXI"},"outputs":[],"source":["class_mapping = {\n","      \"Face\": 0,\n","      \"Leye\": 1,\n","      \"Reye\": 2,\n","      \"Mouth\": 3,\n","      \"Cigar\": 4,\n","      \"Phone\": 5\n","}"]},{"cell_type":"markdown","metadata":{"id":"I7hP6Lfr6amh"},"source":["이미지에서 양쪽 눈과 입 사진을 잘라서 저장한다.\n","이때 파일명은 기존 파일명_클래스 이름_Open 혹은 기존 파일명_클래스 이름_Close로 한다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vh6V_BqCvjHv"},"outputs":[],"source":["for image in os.listdir(jpeg_dir):\n","  if image.endswith('jpg'):\n","    name = os.path.splitext(image)[0]\n","\n","    img_path = os.path.join(jpeg_dir, image)\n","    annot_path = os.path.join(annot_dir, name + '.json')\n","\n","    with open(annot_path, 'r', encoding='utf-8') as f:\n","      data = json.load(f)\n","\n","    for obj, bbox in data[\"ObjectInfo\"][\"BoundingBox\"].items():\n","      if bbox[\"isVisible\"] and class_mapping[obj] in [1, 2, 3]:\n","\n","        x1, y1, x2, y2 = bbox[\"Position\"]\n","\n","        Is_open = \"Open\" if bbox[\"Opened\"] else \"Close\"\n","\n","        img = cv2.imread(img_path)\n","        crop_img = img[y1:y2, x1:x2]\n","\n","        img_dst = os.path.join(eyes_mouth, f\"{name}_{obj}_{Is_open}.jpg\")\n","        cv2.imwrite(img_dst, crop_img)"]},{"cell_type":"markdown","metadata":{"id":"bvDgDHKJ3zKG"},"source":["파일명을 참고하여 이미지를 open 혹은 close 폴더로 옮긴다.\n","\n","\n","\n","```python\n","# /content/drive/MyDrive/CAB/CAB_dataset/fs/\n","# ├── open/\n","# │   └── 파일명_클래스 이름_Open.jpg\n","# ├── close/\n","#      └── 파일명_클래스 이름_Close.jpg\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LUZfhCEi3_bO"},"outputs":[],"source":["data_root = \"/content/drive/MyDrive/CAB/CAB_dataset/\"\n","\n","fs_root = os.path.join(data_root, \"fs\")\n","open_root = os.path.join(fs_root, \"open\")\n","close_root = os.path.join(fs_root, \"close\")\n","\n","os.makedirs(open_root, exist_ok=False)\n","os.makedirs(close_root, exist_ok=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":210},"executionInfo":{"elapsed":327,"status":"error","timestamp":1733968106210,"user":{"displayName":"이야호","userId":"06785888825932324883"},"user_tz":-540},"id":"oZyD0U5H4VmH","outputId":"b886c99b-5f3c-4777-8a34-a1e43a181342"},"outputs":[{"ename":"NameError","evalue":"name 'eyes_mouth' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-35-69d69685ad3d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meyes_mouth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'Open'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mimg_src\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meyes_mouth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mimg_dst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'eyes_mouth' is not defined"]}],"source":["for file in os.listdir(eyes_mouth):\n","  if file.endswith('.jpg') and 'Open' in file:\n","    img_src = os.path.join(eyes_mouth, file)\n","    img_dst = os.path.join(open_root, file)\n","\n","  elif file.endswith('.jpg') and 'Close' in file:\n","    img_src = os.path.join(eyes_mouth, file)\n","    img_dst = os.path.join(close_root, file)\n","\n","  shutil.copy2(img_src, img_dst)\n"]},{"cell_type":"markdown","metadata":{"id":"ZKkNWBujF9fr"},"source":["**데이터 undersampling**"]},{"cell_type":"markdown","metadata":{"id":"F4WkYpwj8OIQ"},"source":["다음으로, 모든 Image의 절대 경로가 적힌 리스트를 만든다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"diWxowLwmezv"},"outputs":[],"source":["data_root = \"/content/drive/MyDrive/CAB/CAB_dataset/\"\n","\n","fs_root = os.path.join(data_root, \"fs\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":66086,"status":"ok","timestamp":1735187770000,"user":{"displayName":"정서영","userId":"00670050857396282545"},"user_tz":-540},"id":"bqOahdk_F7A5","outputId":"3a33438f-a452-45d8-e4da-fdb60df2bf9f"},"outputs":[{"output_type":"stream","name":"stdout","text":["클래스별 인덱스: {'close': 0, 'open': 1}\n","클래스별 샘플 수: [8661, 15364]\n","데이터셋 크기: 24025\n"]}],"source":["transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.RandomRotation(degrees=(30)),\n","    transforms.RandomVerticalFlip(p=0.5),\n","    transforms.GaussianBlur(kernel_size = 5, sigma = (0.5, 2.0)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","dataset = ImageFolder(root=fs_root, transform=transform)\n","\n","class_to_idx = dataset.class_to_idx\n","print(f'클래스별 인덱스: {class_to_idx}')\n","\n","class_counts = [0] * len(class_to_idx)\n","for _, target in dataset.samples:\n","    class_counts[target] += 1\n","\n","print(f\"클래스별 샘플 수: {class_counts}\")\n","print(f\"데이터셋 크기: {len(dataset)}\")\n"]},{"cell_type":"markdown","metadata":{"id":"_Vc-7akjC64s"},"source":["추가 코드\n","\n","close eye와 close mouth의 샘플 수 비율은 비슷하지만 open eye 샘플 수가 open mouth 샘플 수의 약 6배이다."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1735187770000,"user":{"displayName":"정서영","userId":"00670050857396282545"},"user_tz":-540},"id":"6Y4Gdv3a_zW9","outputId":"bfe3756e-36e5-460d-d835-da8d387db76a"},"outputs":[{"output_type":"stream","name":"stdout","text":["close samples: 8661\n","open samples: 15364\n"]}],"source":["close_samples = []\n","open_samples = []\n","\n","for sample, target in dataset.samples:\n","  if target == 0:\n","    close_samples.append((sample, target))\n","  else:\n","    open_samples.append((sample, target))\n","\n","print(f\"close samples: {len(close_samples)}\")\n","print(f\"open samples: {len(open_samples)}\")"]},{"cell_type":"markdown","metadata":{"id":"8F-b8_vqAK2H"},"source":["데이터 불균형 문제와 제공된 GPU RAM, 학습 속도 등을 고려하여 클래스별 샘플 수를 줄여준다.\n","\n","한 가지 추가적으로 고려해야 할 점은, 각 class 안의 eye sample과 mouth sample의 비율이다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ugS2Vim_qQic"},"outputs":[],"source":["e_close = []\n","e_open = []\n","m_close = []\n","m_open = []"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zysQLeGvpOLH"},"outputs":[],"source":["for sample, target in dataset.samples :\n","  if target == 0:\n","    if 'eye' in sample:\n","      e_close.append((sample, target))\n","    if 'Mouth' in sample:\n","      m_close.append((sample, target))\n","  else:\n","    if 'eye' in sample:\n","      e_open.append((sample, target))\n","    if 'Mouth' in sample:\n","      m_open.append((sample, target))"]},{"cell_type":"markdown","metadata":{"id":"ms2pUz7drvFM"},"source":["보다시피, open class 안에 eye sample이 mouth sample보다 약 6배는 더 많은 것을 알 수 있다."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1735187770001,"user":{"displayName":"정서영","userId":"00670050857396282545"},"user_tz":-540},"id":"wL0RGI8-qgBw","outputId":"b61ec375-84cd-4a8b-aa06-eaaa9f900151"},"outputs":[{"output_type":"stream","name":"stdout","text":["close eye samples: 4593\n","close mouth samples: 4068\n","open eye samples: 12898\n","open mouth samples: 2466\n"]}],"source":["print(f\"close eye samples: {len(e_close)}\")\n","print(f\"close mouth samples: {len(m_close)}\")\n","print(f\"open eye samples: {len(e_open)}\")\n","print(f\"open mouth samples: {len(m_open)}\")"]},{"cell_type":"markdown","metadata":{"id":"rTf1bsf5r_iX"},"source":["따라서 각 class의 비율뿐만 아니라 class 안의 mouth, eye sample이 비율도 고려하여 dataset을 나눈다.\n","\n","모델의 최종 inference 결과를 봤을 때, 졸음에 대한 recall 값이 상당히 낮았다. 따라서 각 class에 입 사진보다 눈 사진을 더 많이 포함하였다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LEB1HkCDsMs2"},"outputs":[],"source":["random.shuffle(e_close)\n","random.shuffle(e_open)\n","random.shuffle(m_close)\n","random.shuffle(m_open)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DUTFAnrZsRps"},"outputs":[],"source":["close_samples = e_close[:1000] + e_close[3343:] + m_close[:1750]\n","open_samples = e_open[:1000] + e_open[11648:] + m_open[:1750]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o_D4Y5_eXUT1"},"outputs":[],"source":["dataset.samples = close_samples + open_samples"]},{"cell_type":"markdown","metadata":{"id":"N-r0VJK1rG6d"},"source":["**train/val/test loader 생성**"]},{"cell_type":"markdown","metadata":{"id":"sd-HtlAr84WI"},"source":["전체 이미지를 6:2:2 비율로 나누어 train set, validation set, test set을 생성한다.\n","\n","ImageFolder는 instance를 직접 섞는 걸 허용하지 않는다. 따라서 무작위로 섞인 인덱스를 이용하여 훈련/검증/테스트 데이터셋을 만든다.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yrBh8t65pCzF"},"outputs":[],"source":["dataset_indices = list(range(len(dataset)))\n","\n","random.shuffle(dataset_indices)\n","\n","train_size = int(0.6 * len(dataset))\n","val_size = int(0.2 * len(dataset))\n","test_size = val_size\n","\n","train_dataset = Subset(dataset, dataset_indices[:train_size])\n","val_dataset = Subset(dataset, dataset_indices[train_size:train_size+val_size])\n","test_dataset = Subset(dataset, dataset_indices[train_size+val_size:])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1735187770001,"user":{"displayName":"정서영","userId":"00670050857396282545"},"user_tz":-540},"id":"QUoDyBso87dh","outputId":"58034be8-0a38-466d-de58-10bcfa1d2f73"},"outputs":[{"output_type":"stream","name":"stdout","text":["length of train dataset:  4800\n","length of val dataset:  1600\n","length of test dataset:  1600\n"]}],"source":["print(\"length of train dataset: \", len(train_dataset))\n","print(\"length of val dataset: \", len(val_dataset))\n","print(\"length of test dataset: \", len(test_dataset))"]},{"cell_type":"markdown","metadata":{"id":"bfDBuE8X9K8N"},"source":["다음으로, 훈련/검증/테스트 데이터로더를 생성한다.\n","\n","- num_workers: 2개의 프로세서가 병렬로 데이터를 불러와 이력 데이터가 더 빨리 준비될 수 있도록 한다.\n","- pin_memory=True: CPU에서 GPU로 데이터를 전송할 때 발생하는 복사 작업을 빠르게 할 수 있도록 도와준다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UFQ3W3NwFg7k"},"outputs":[],"source":["# 데이터 로더 생성\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, pin_memory=True, num_workers=2)\n","val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True, pin_memory=True, num_workers=2)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True, pin_memory=True, num_workers=2)"]},{"cell_type":"markdown","metadata":{"id":"Glrb9tv0FTuh"},"source":["**분류 모델 생성**"]},{"cell_type":"markdown","metadata":{"id":"UpOtc48YpnQ3"},"source":["분류 모델로는 이전 프로젝트에서 우수한 성능을 보였던 ResNet50 모델을 선택하였다.\n","\n","다만 이번 프로젝트에서는 ResNet50 모델에 compound scaling을 적용해 보았다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PkxtKEKH6tNm"},"outputs":[],"source":["# SE block + compound scaling을 적용한 ResNet50 훈련\n","\n","\n","# class SEBlock(nn.Module):\n","#     def __init__(self, in_channels, reduction=16):\n","#         super(SEBlock, self).__init__()\n","#         self.pool = nn.AdaptiveAvgPool2d(1)\n","#         self.conv1 = nn.Conv2d(in_channels, in_channels // reduction, kernel_size=1)\n","#         self.activ = nn.ReLU(inplace=True)\n","#         self.conv2= nn.Conv2d(in_channels // reduction, in_channels, kernel_size=1)\n","#         self.sigmoid = nn.Sigmoid()\n","\n","#     def forward(self, x):\n","#         w = self.pool(x)\n","#         w = self.conv1(w)\n","#         w = self.activ(w)\n","#         w = self.conv2(w)\n","#         w = self.sigmoid(w)\n","#         x = x * w\n","\n","#         return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bYtxbUidmhwF"},"outputs":[],"source":["# class CompoundScaledResNet50(nn.Module):\n","#   def __init__(self, width_mult=1.1, depth_mult=1.2, resolution_mult=1.15):\n","#     super(CompoundScaledResNet50, self).__init__()\n","#     self.base_model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n","\n","#     self.width_mult = width_mult\n","#     self.depth_mult = depth_mult\n","#     self.resolution_mult = resolution_mult\n","\n","#     self.input_resolution = int(224 * resolution_mult) # increase the resolution of images\n","\n","#     #self._modify_initial_layers()\n","#     self._apply_width_scaling_to_bottlenecks()\n","#     # self._modify_downsample_layers()\n","#     self._apply_depth_scaling()\n","\n","#   def _apply_width_scaling_to_bottlenecks(self):\n","#     for name, module in self.base_model.named_modules():\n","#       if isinstance(module, models.resnet.Bottleneck):\n","\n","#         orig_conv2 = module.conv2 # middle layer of bottleneck\n","\n","#         scaled_channels = int(orig_conv2.out_channels * self.width_mult) # increase the channel of images\n","\n","#         new_conv1 = nn.Conv2d(\n","#             in_channels = module.conv1.in_channels, # keep original input\n","#             out_channels = scaled_channels,\n","#             kernel_size = 1,\n","#             stride = module.conv1.stride,\n","#             bias = False\n","#         )\n","\n","#         new_conv2 = nn.Conv2d(\n","#             in_channels = scaled_channels,\n","#             out_channels = scaled_channels,\n","#             kernel_size = 3,\n","#             stride = module.conv2.stride,\n","#             padding = 1,\n","#             bias = False\n","#         )\n","\n","#         new_conv3 = nn.Conv2d(\n","#             in_channels = scaled_channels,\n","#             out_channels = module.conv3.out_channels,\n","#             kernel_size = 1,\n","#             bias = False\n","#         )\n","\n","#         new_weight1 = torch.randn(scaled_channels, module.conv1.in_channels, 1, 1)\n","#         new_weight2 = torch.randn(scaled_channels, scaled_channels, 3, 3)\n","#         new_weight3 = torch.randn(module.conv3.out_channels, scaled_channels, 1, 1)\n","\n","#         init.kaiming_uniform_(new_weight1, a=math.sqrt(5))\n","#         init.kaiming_uniform_(new_weight2, a=math.sqrt(5))\n","#         init.kaiming_uniform_(new_weight3, a=math.sqrt(5))\n","\n","#         new_conv1.weight.data = new_weight1\n","#         new_conv2.weight.data = new_weight2\n","#         new_conv3.weight.data = new_weight3\n","\n","#         new_bn1 = nn.BatchNorm2d(scaled_channels)\n","#         new_bn2 = nn.BatchNorm2d(scaled_channels)\n","#         new_bn3 = nn.BatchNorm2d(module.conv3.out_channels)\n","\n","#         module.conv1 = new_conv1\n","#         module.bn1 = new_bn1\n","\n","#         module.conv2 = new_conv2\n","#         module.bn2 = new_bn2\n","\n","#         module.conv3 = new_conv3\n","#         module.bn3 = new_bn3\n","\n","#         if (module.downsample):\n","#           downsample_module = module.downsample\n","#           del module.downsample\n","\n","#           module.se_block = SEBlock(module.conv3.out_channels)\n","#           module.downsample = downsample_module\n","\n","#         else:\n","#           module.se_block = SEBlock(module.conv3.out_channels)\n","\n","#   def _modify_downsample_layers(self):\n","#     for name, module in self.base_model.named_modules():\n","#       if isinstance(module, models.resnet.Bottleneck) and module.downsample is not None:\n","#         conv1_in_channels = module.conv1.in_channels\n","\n","#         if hasattr(module, 'downsample'):\n","#           module.downsample = nn.Sequential(\n","#               nn.Conv2d(\n","#                   in_channels = conv1_in_channels,\n","#                   out_channels = module.downsample[0].out_channels,\n","#                   kernel_size = 1,\n","#                   stride = module.downsample[0].stride,\n","#                   bias = False\n","#               ),\n","#               nn.BatchNorm2d(module.downsample[0].out_channels)\n","#           )\n","\n","#   def _apply_depth_scaling(self):\n","#     for name, child in self.base_model.named_children():\n","#       if isinstance(child, nn.Sequential):\n","#         new_depth = math.ceil(len(child) * self.depth_mult) # increase the depth\n","\n","#         if new_depth > len(child):\n","#           additional_layers = []\n","\n","#           for _ in range(new_depth - len(child)):\n","#             new_layer = self._modify_for_dilated_conv(copy.deepcopy(child[-1]))\n","#             additional_layers.append(new_layer)\n","\n","#           setattr(self.base_model, name, nn.Sequential(*list(child), *additional_layers)) # original seq + added layers\n","\n","\n","#   def _modify_for_dilated_conv(self, bottleneck, dilation=2):\n","#     b = bottleneck\n","\n","#     if isinstance(b.conv2, nn.Conv2d):\n","#       b.conv2 = nn.Conv2d(\n","#           in_channels = b.conv2.in_channels,\n","#           out_channels = b.conv2.out_channels,\n","#           kernel_size = b.conv2.kernel_size,\n","#           stride = b.conv2.stride,\n","#           padding = (b.conv2.kernel_size[0] // 2) * dilation,\n","#           dilation = dilation\n","#       )\n","#     return b\n","\n","#   def forward(self, x):\n","#     x = nn.functional.interpolate(x, size=(self.input_resolution, self.input_resolution), mode='bilinear')\n","\n","#     return self.base_model(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7k1CQKMq45Ai"},"outputs":[],"source":["class CompoundScaledResNet50(nn.Module):\n","  def __init__(self, width_mult=1.1, depth_mult=1.2, resolution_mult=1.15):\n","    super(CompoundScaledResNet50, self).__init__()\n","    self.base_model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n","\n","    self.width_mult = width_mult\n","    self.depth_mult = depth_mult\n","    self.resolution_mult = resolution_mult\n","\n","    self.input_resolution = int(224 * resolution_mult) # increase the resolution of images\n","\n","    #self._modify_initial_layers()\n","    self._apply_width_scaling_to_bottlenecks()\n","    # self._modify_downsample_layers()\n","    self._apply_depth_scaling()\n","\n","  def _apply_width_scaling_to_bottlenecks(self):\n","    for name, module in self.base_model.named_modules():\n","      if isinstance(module, models.resnet.Bottleneck):\n","\n","        orig_conv2 = module.conv2 # middle layer of bottleneck\n","\n","        scaled_channels = int(orig_conv2.out_channels * self.width_mult) # increase the channel of images\n","\n","        new_conv1 = nn.Conv2d(\n","            in_channels = module.conv1.in_channels, # keep original input\n","            out_channels = scaled_channels,\n","            kernel_size = 1,\n","            stride = module.conv1.stride,\n","            bias = False\n","        )\n","\n","        new_conv2 = nn.Conv2d(\n","            in_channels = scaled_channels,\n","            out_channels = scaled_channels,\n","            kernel_size = 3,\n","            stride = module.conv2.stride,\n","            padding = 1,\n","            bias = False\n","        )\n","\n","        new_conv3 = nn.Conv2d(\n","            in_channels = scaled_channels,\n","            out_channels = module.conv3.out_channels,\n","            kernel_size = 1,\n","            bias = False\n","        )\n","\n","        new_weight1 = torch.randn(scaled_channels, module.conv1.in_channels, 1, 1)\n","        new_weight2 = torch.randn(scaled_channels, scaled_channels, 3, 3)\n","        new_weight3 = torch.randn(module.conv3.out_channels, scaled_channels, 1, 1)\n","\n","        init.kaiming_uniform_(new_weight1, a=math.sqrt(5))\n","        init.kaiming_uniform_(new_weight2, a=math.sqrt(5))\n","        init.kaiming_uniform_(new_weight3, a=math.sqrt(5))\n","\n","        new_conv1.weight.data = new_weight1\n","        new_conv2.weight.data = new_weight2\n","        new_conv3.weight.data = new_weight3\n","\n","        new_bn1 = nn.BatchNorm2d(scaled_channels)\n","        new_bn2 = nn.BatchNorm2d(scaled_channels)\n","        new_bn3 = nn.BatchNorm2d(module.conv3.out_channels)\n","\n","        module.conv1 = new_conv1\n","        module.bn1 = new_bn1\n","\n","        module.conv2 = new_conv2\n","        module.bn2 = new_bn2\n","\n","        module.conv3 = new_conv3\n","        module.bn3 = new_bn3\n","\n","  def _modify_downsample_layers(self):\n","    for name, module in self.base_model.named_modules():\n","      if isinstance(module, models.resnet.Bottleneck) and module.downsample is not None:\n","        conv1_in_channels = module.conv1.in_channels\n","\n","        if hasattr(module, 'downsample'):\n","          module.downsample = nn.Sequential(\n","              nn.Conv2d(\n","                  in_channels = conv1_in_channels,\n","                  out_channels = module.downsample[0].out_channels,\n","                  kernel_size = 1,\n","                  stride = module.downsample[0].stride,\n","                  bias = False\n","              ),\n","              nn.BatchNorm2d(module.downsample[0].out_channels)\n","          )\n","\n","  def _apply_depth_scaling(self):\n","    for name, child in self.base_model.named_children():\n","      if isinstance(child, nn.Sequential):\n","        new_depth = math.ceil(len(child) * self.depth_mult) # increase the depth\n","\n","        if new_depth > len(child):\n","          additional_layers = []\n","\n","          for _ in range(new_depth - len(child)):\n","            new_layer = self._modify_for_dilated_conv(copy.deepcopy(child[-1]))\n","            additional_layers.append(new_layer)\n","\n","          setattr(self.base_model, name, nn.Sequential(*list(child), *additional_layers)) # original seq + added layers\n","\n","\n","  def _modify_for_dilated_conv(self, bottleneck, dilation=2):\n","    b = bottleneck\n","\n","    if isinstance(b.conv2, nn.Conv2d):\n","      b.conv2 = nn.Conv2d(\n","          in_channels = b.conv2.in_channels,\n","          out_channels = b.conv2.out_channels,\n","          kernel_size = b.conv2.kernel_size,\n","          stride = b.conv2.stride,\n","          padding = (b.conv2.kernel_size[0] // 2) * dilation,\n","          dilation = dilation\n","      )\n","    return b\n","\n","  def forward(self, x):\n","    x = nn.functional.interpolate(x, size=(self.input_resolution, self.input_resolution), mode='bilinear')\n","\n","    return self.base_model(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o_9ntHZek6lQ"},"outputs":[],"source":["class MyCompoundScaledResNet50(nn.Module):\n","  def __init__(self, width_mult=1.1, depth_mult=1.2, resolution_mult=1.15):\n","    super(MyCompoundScaledResNet50, self).__init__()\n","\n","    self.backbone = CompoundScaledResNet50(\n","        width_mult=width_mult,\n","        depth_mult=depth_mult,\n","        resolution_mult=resolution_mult\n","    )\n","\n","    self.dropout = nn.Dropout(0.3)\n","    self.extra_layer = nn.Linear(1000, 2)\n","\n","  def forward(self, x):\n","    x = self.backbone(x)\n","    x = self.dropout(x)\n","    x = self.extra_layer(x)\n","    return x"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3439,"status":"ok","timestamp":1735187773434,"user":{"displayName":"정서영","userId":"00670050857396282545"},"user_tz":-540},"id":"hEl9uq5ljbFW","outputId":"0469b21b-fcb2-4d8b-dd19-6fbcc4460b9a"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n","100%|██████████| 97.8M/97.8M [00:01<00:00, 96.0MB/s]\n"]}],"source":["c_resnet = MyCompoundScaledResNet50().cuda()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1735187773435,"user":{"displayName":"정서영","userId":"00670050857396282545"},"user_tz":-540},"id":"dWxsORlivErt","outputId":"0c20c6de-0639-4f0c-a144-e80178cf278f"},"outputs":[{"output_type":"stream","name":"stdout","text":[" MyCompoundScaledResNet50(\n","  (backbone): CompoundScaledResNet50(\n","    (base_model): ResNet(\n","      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","      (layer1): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(64, 70, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(70, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(70, 70, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(70, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(70, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(256, 70, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(70, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(70, 70, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(70, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(70, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(256, 70, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(70, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(70, 70, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(70, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(70, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(256, 70, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(70, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(70, 70, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n","          (bn2): BatchNorm2d(70, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(70, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer2): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(256, 140, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(140, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(140, 140, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(140, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(140, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(512, 140, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(140, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(140, 140, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(140, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(140, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(512, 140, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(140, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(140, 140, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(140, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(140, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(512, 140, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(140, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(140, 140, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(140, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(140, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (4): Bottleneck(\n","          (conv1): Conv2d(512, 140, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(140, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(140, 140, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n","          (bn2): BatchNorm2d(140, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(140, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer3): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(512, 281, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(281, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(281, 281, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(281, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(281, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(1024, 281, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(281, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(281, 281, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(281, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(281, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(1024, 281, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(281, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(281, 281, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(281, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(281, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(1024, 281, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(281, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(281, 281, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(281, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(281, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (4): Bottleneck(\n","          (conv1): Conv2d(1024, 281, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(281, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(281, 281, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(281, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(281, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (5): Bottleneck(\n","          (conv1): Conv2d(1024, 281, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(281, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(281, 281, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(281, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(281, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (6): Bottleneck(\n","          (conv1): Conv2d(1024, 281, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(281, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(281, 281, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n","          (bn2): BatchNorm2d(281, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(281, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (7): Bottleneck(\n","          (conv1): Conv2d(1024, 281, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(281, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(281, 281, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n","          (bn2): BatchNorm2d(281, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(281, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer4): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(1024, 563, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(563, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(563, 563, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(563, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(563, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(2048, 563, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(563, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(563, 563, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(563, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(563, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(2048, 563, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(563, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(563, 563, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(563, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(563, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(2048, 563, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(563, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(563, 563, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n","          (bn2): BatchNorm2d(563, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(563, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","      (fc): Linear(in_features=2048, out_features=1000, bias=True)\n","    )\n","  )\n","  (dropout): Dropout(p=0.3, inplace=False)\n","  (extra_layer): Linear(in_features=1000, out_features=2, bias=True)\n",")\n"]}],"source":["# \"modules\"는 자신에게 속하는 모든 submodule들을 표시\n","# \"children\"은 한 단계 아래의 submodule까지만 표시\n","# https://data-scientist-han.tistory.com/110\n","\n","for name, module in c_resnet.named_modules():\n","    print(name, module)\n","    break"]},{"cell_type":"markdown","metadata":{"id":"wjtmw_pGyuvj"},"source":["학습이 불안정할 때 더 빠르고 안정적인 수렴을 유도하기 위해 NAdam optimizer를 사용하였고, 학습 후반부에 과적합을 방지하면서 세밀하게 학습할 수 있도록 CosineAnnealingLR도 사용하였다.\n","\n","또한 모델의 일반화 성능을 향상시키기 위해 l2 정규화를 적용하였다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pG2wjXOFkJVA"},"outputs":[],"source":["optimizer = optim.NAdam(c_resnet.parameters(), lr=0.001, weight_decay=1e-5)\n","scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=40, eta_min=0)\n","\n","criterion = nn.CrossEntropyLoss()\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"markdown","metadata":{"id":"3AsVQaiUFNpi"},"source":["**훈련/검증/테스트 함수 정의**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QksJsWa3HADz"},"outputs":[],"source":["def train(model):\n","  model.train()\n","\n","  for batch_idx, (data, target) in enumerate(train_loader):\n","    data, target = data.to(device), target.to(device)\n","\n","    optimizer.zero_grad()\n","\n","    output = model(data)\n","\n","    loss = criterion(output, target)\n","    loss.backward()\n","\n","    optimizer.step()\n","  scheduler.step()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EDf95mlmqHaF"},"outputs":[],"source":["def val(model, epoch):\n","  model.eval()\n","\n","  val_loss = 0\n","  correct = 0\n","\n","  with torch.no_grad():\n","    for data, target in val_loader:\n","      data, target = data.to(device), target.to(device)\n","\n","      output = model(data)\n","\n","      val_loss += criterion(output, target).item()\n","      pred = output.argmax(dim=1, keepdim=True)\n","      correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","  val_loss /= len(val_loader.dataset)\n","  accuracy = 100. * correct / len(val_loader.dataset)\n","\n","  print(f\"Epoch: {epoch}, Average loss: {val_loss:.4f}, Accuracy: {correct}/{len(val_loader.dataset)} ({accuracy:.2f}%)\")\n","\n","  return val_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n3Ixf3LoqK98"},"outputs":[],"source":["def test(model):\n","  model.eval()\n","\n","  test_loss = 0\n","  correct = 0\n","\n","  with torch.no_grad():\n","    for data, target in test_loader:\n","      data, target = data.to(device), target.to(device)\n","\n","      output = model(data)\n","\n","      test_loss += criterion(output, target).item()\n","      pred = output.argmax(dim=1, keepdim=True)\n","      correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","  test_loss /= len(test_loader.dataset)\n","  accuracy = 100. * correct / len(test_loader.dataset)\n","\n","  print(f\"Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)\")\n"]},{"cell_type":"markdown","metadata":{"id":"JTFcBL6jrOdy"},"source":["**EarlyStopping 정의**"]},{"cell_type":"markdown","metadata":{"id":"X4bLt8dmFSDW"},"source":["overfitting을 방지하기 위해 earlystopping을 사용한다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tWzOvaWMHGqB"},"outputs":[],"source":["class EarlyStopping:\n","    def __init__(self, patience=13, verbose=False, counter=0, best_loss=float('inf')):\n","        self.patience = patience\n","        self.verbose = verbose\n","        self.counter = counter\n","        self.best_loss = best_loss\n","        self.early_stop = False\n","\n","    def __call__(self, val_loss, model):\n","        if val_loss < self.best_loss:\n","            self.counter = 0\n","            self.best_loss = val_loss\n","            torch.save(model.state_dict(), '/content/drive/MyDrive/CAB/CAB_dataset/model/best_decay_model.pth')\n","        else:\n","            self.counter += 1\n","            if self.counter >= self.patience:\n","              self.early_stop = True"]},{"cell_type":"markdown","source":["**모델 학습 및 평가**"],"metadata":{"id":"4KX0XVfTbGtL"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ugVjepZCGLPD"},"outputs":[],"source":["num_epochs = 200"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7fEjkFl96X_C","outputId":"bf595e2b-89ca-4c17-9664-c692a6f3a633","executionInfo":{"status":"ok","timestamp":1735195711755,"user_tz":-540,"elapsed":7938325,"user":{"displayName":"정서영","userId":"00670050857396282545"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1, Average loss: 0.0212, Accuracy: 1065/1600 (66.56%)\n","Epoch: 2, Average loss: 0.0341, Accuracy: 1010/1600 (63.12%)\n","Epoch: 3, Average loss: 0.0162, Accuracy: 1230/1600 (76.88%)\n","Epoch: 4, Average loss: 0.0167, Accuracy: 1201/1600 (75.06%)\n","Epoch: 5, Average loss: 0.0180, Accuracy: 1140/1600 (71.25%)\n","Epoch: 6, Average loss: 0.0127, Accuracy: 1319/1600 (82.44%)\n","Epoch: 7, Average loss: 0.0306, Accuracy: 1097/1600 (68.56%)\n","Epoch: 8, Average loss: 0.0151, Accuracy: 1252/1600 (78.25%)\n","Epoch: 9, Average loss: 0.0313, Accuracy: 1109/1600 (69.31%)\n","Epoch: 10, Average loss: 0.0275, Accuracy: 1125/1600 (70.31%)\n","Epoch: 11, Average loss: 0.0125, Accuracy: 1330/1600 (83.12%)\n","Epoch: 12, Average loss: 0.0214, Accuracy: 1157/1600 (72.31%)\n","Epoch: 13, Average loss: 0.0116, Accuracy: 1337/1600 (83.56%)\n","Epoch: 14, Average loss: 0.0161, Accuracy: 1291/1600 (80.69%)\n","Epoch: 15, Average loss: 0.0100, Accuracy: 1388/1600 (86.75%)\n","Epoch: 16, Average loss: 0.0076, Accuracy: 1453/1600 (90.81%)\n","Epoch: 17, Average loss: 0.0091, Accuracy: 1420/1600 (88.75%)\n","Epoch: 18, Average loss: 0.0128, Accuracy: 1361/1600 (85.06%)\n","Epoch: 19, Average loss: 0.0105, Accuracy: 1380/1600 (86.25%)\n","Epoch: 20, Average loss: 0.0082, Accuracy: 1458/1600 (91.12%)\n","Epoch: 21, Average loss: 0.0059, Accuracy: 1483/1600 (92.69%)\n","Epoch: 22, Average loss: 0.0071, Accuracy: 1476/1600 (92.25%)\n","Epoch: 23, Average loss: 0.0055, Accuracy: 1493/1600 (93.31%)\n","Epoch: 24, Average loss: 0.0055, Accuracy: 1492/1600 (93.25%)\n","Epoch: 25, Average loss: 0.0109, Accuracy: 1410/1600 (88.12%)\n","Epoch: 26, Average loss: 0.0055, Accuracy: 1503/1600 (93.94%)\n","Epoch: 27, Average loss: 0.0055, Accuracy: 1489/1600 (93.06%)\n","Epoch: 28, Average loss: 0.0064, Accuracy: 1484/1600 (92.75%)\n","Epoch: 29, Average loss: 0.0046, Accuracy: 1521/1600 (95.06%)\n","Epoch: 30, Average loss: 0.0047, Accuracy: 1518/1600 (94.88%)\n","Epoch: 31, Average loss: 0.0056, Accuracy: 1499/1600 (93.69%)\n","Epoch: 32, Average loss: 0.0049, Accuracy: 1517/1600 (94.81%)\n","Epoch: 33, Average loss: 0.0047, Accuracy: 1517/1600 (94.81%)\n","Epoch: 34, Average loss: 0.0046, Accuracy: 1523/1600 (95.19%)\n","Epoch: 35, Average loss: 0.0044, Accuracy: 1526/1600 (95.38%)\n","Epoch: 36, Average loss: 0.0044, Accuracy: 1524/1600 (95.25%)\n","Epoch: 37, Average loss: 0.0043, Accuracy: 1526/1600 (95.38%)\n","Epoch: 38, Average loss: 0.0045, Accuracy: 1524/1600 (95.25%)\n","Epoch: 39, Average loss: 0.0047, Accuracy: 1518/1600 (94.88%)\n","Epoch: 40, Average loss: 0.0047, Accuracy: 1522/1600 (95.12%)\n","Epoch: 41, Average loss: 0.0044, Accuracy: 1530/1600 (95.62%)\n","Epoch: 42, Average loss: 0.0045, Accuracy: 1522/1600 (95.12%)\n","Epoch: 43, Average loss: 0.0043, Accuracy: 1527/1600 (95.44%)\n","Epoch: 44, Average loss: 0.0049, Accuracy: 1522/1600 (95.12%)\n","Epoch: 45, Average loss: 0.0049, Accuracy: 1517/1600 (94.81%)\n","Epoch: 46, Average loss: 0.0046, Accuracy: 1518/1600 (94.88%)\n","Epoch: 47, Average loss: 0.0051, Accuracy: 1520/1600 (95.00%)\n","Epoch: 48, Average loss: 0.0047, Accuracy: 1513/1600 (94.56%)\n","Epoch: 49, Average loss: 0.0043, Accuracy: 1522/1600 (95.12%)\n","Epoch: 50, Average loss: 0.0045, Accuracy: 1514/1600 (94.62%)\n","************************************************************\n","Early stop!\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-26-e0aed74a9c65>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  c_resnet.load_state_dict(torch.load('/content/drive/MyDrive/CAB/CAB_dataset/model/best_decay_model.pth', map_location=device))\n"]},{"output_type":"stream","name":"stdout","text":["Average loss: 0.0043, Accuracy: 1529/1600 (95.56%)\n"]}],"source":["early_stopping = EarlyStopping(verbose=True)\n","\n","for epoch in range(1, num_epochs + 1):\n","  train(c_resnet)\n","  val_loss = val(c_resnet, epoch)\n","\n","  early_stopping(val_loss, c_resnet)\n","\n","  if early_stopping.early_stop:\n","    print(\"************************************************************\\nEarly stop!\")\n","    c_resnet.load_state_dict(torch.load('/content/drive/MyDrive/CAB/CAB_dataset/model/best_decay_model.pth', map_location=device))\n","    test(c_resnet)\n","    break"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}