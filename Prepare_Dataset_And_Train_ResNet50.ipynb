{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOZxEgP70BrmIMFrU5ng9vk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["**라이브러리 임포트**"],"metadata":{"id":"IRZAusNkkjD6"}},{"cell_type":"code","source":["import os\n","import json\n","import random\n","import shutil\n","import cv2\n","\n","from google.colab import drive\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision.datasets import ImageFolder\n","from torchvision import transforms, models, datasets\n","from torch.utils.data import DataLoader, Subset"],"metadata":{"id":"K83thxLqqJ61","executionInfo":{"status":"ok","timestamp":1731941416642,"user_tz":-540,"elapsed":6509,"user":{"displayName":"정서영","userId":"11814773768061491291"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["**데이터셋 unzip**"],"metadata":{"id":"419v39A8GQ-A"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T_46dgIdpSf4","executionInfo":{"status":"ok","timestamp":1731941444196,"user_tz":-540,"elapsed":23817,"user":{"displayName":"정서영","userId":"11814773768061491291"}},"outputId":"3300d73d-4f88-47fd-9f4e-cac6f3088365"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["drive.mount('/content/drive')"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/CAB/CAB_dataset/JPEGImages\n","!unzip -qq \"/content/drive/MyDrive/CAB/CAB_dataset/JPEGImages/images_bbox.zip\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XjLJ0hM1qB0H","executionInfo":{"status":"ok","timestamp":1731672852929,"user_tz":-540,"elapsed":201295,"user":{"displayName":"정서영","userId":"11814773768061491291"}},"outputId":"437f43f0-9064-4c77-f8a6-e28803254424"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/CAB/CAB_dataset/JPEGImages\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/CAB/CAB_dataset/Annotations\n","!unzip -qq \"/content/drive/MyDrive/CAB/CAB_dataset/Annotations/label_bbox.zip\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NgqLPUCVqDXJ","executionInfo":{"status":"ok","timestamp":1731673005280,"user_tz":-540,"elapsed":152358,"user":{"displayName":"정서영","userId":"11814773768061491291"}},"outputId":"9e607d12-e49c-4d8a-a2ec-8ef30e0b8dec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/CAB/CAB_dataset/Annotations\n"]}]},{"cell_type":"markdown","source":["**데이터 라벨링**"],"metadata":{"id":"THY1Br4gq-8P"}},{"cell_type":"markdown","source":["파일 경로 변환\n","- /content/drive/MyDrive/CAB/CAB_dataset/JPEGImages 하위 디렉토리에 있는 이미지를 /content/drive/MyDrive/CAB/CAB_dataset/JPEGImages로 옮긴다.\n","- /content/drive/MyDrive/CAB/CAB_dataset/Annotations 하위 디렉토리에 있는 이미지를 /content/drive/MyDrive/CAB/CAB_dataset/Annotations로 옮긴다."],"metadata":{"id":"aiBVR9Oou_Uu"}},{"cell_type":"code","source":["def move_file(base_dir):\n","  for root, dirs, files in os.walk(base_dir):\n","    for file in files:\n","        if file.lower().endswith(('.jpg', 'json')):\n","            source_path = os.path.join(root, file)\n","            destination_path = os.path.join(base_dir, file)\n","\n","            shutil.move(source_path, destination_path)\n","\n","jpeg_dir = \"/content/drive/MyDrive/CAB/CAB_dataset/JPEGImages\"\n","annot_dir = \"/content/drive/MyDrive/CAB/CAB_dataset/Annotations\"\n","\n","move_file(jpeg_dir)\n","move_file(annot_dir)"],"metadata":{"id":"ivkR7gKSuugA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["입과 눈 사진을 저장할 폴더를 만들어 준다."],"metadata":{"id":"zEP_Pgl0kvel"}},{"cell_type":"code","source":["data_root = \"/content/drive/MyDrive/CAB/CAB_dataset/\"\n","eyes_mouth = os.path.join(data_root, \"EyesMouth\")\n","os.makedirs(eyes_mouth, exist_ok=False)"],"metadata":{"id":"sDL7ofFGvBIM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class_mapping = {\n","      \"Face\": 0,\n","      \"Leye\": 1,\n","      \"Reye\": 2,\n","      \"Mouth\": 3,\n","      \"Cigar\": 4,\n","      \"Phone\": 5\n","}"],"metadata":{"id":"2_RDzMQPygXI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["이미지에서 양쪽 눈과 입 사진을 잘라서 저장한다.\n","이때 파일명은 기존 파일명_클래스 이름_Open 혹은 기존 파일명_클래스 이름_Close로 한다."],"metadata":{"id":"I7hP6Lfr6amh"}},{"cell_type":"code","source":["for image in os.listdir(jpeg_dir):\n","  if image.endswith('jpg'):\n","    name = os.path.splitext(image)[0]\n","\n","    img_path = os.path.join(jpeg_dir, image)\n","    annot_path = os.path.join(annot_dir, name + '.json')\n","\n","    with open(annot_path, 'r', encoding='utf-8') as f:\n","      data = json.load(f)\n","\n","    for obj, bbox in data[\"ObjectInfo\"][\"BoundingBox\"].items():\n","      if bbox[\"isVisible\"] and class_mapping[obj] in [1, 2, 3]:\n","\n","        x1, y1, x2, y2 = bbox[\"Position\"]\n","\n","        Is_open = \"Open\" if bbox[\"Opened\"] else \"Close\"\n","\n","        img = cv2.imread(img_path)\n","        crop_img = img[y1:y2, x1:x2]\n","\n","        img_dst = os.path.join(eyes_mouth, f\"{name}_{obj}_{Is_open}.jpg\")\n","        cv2.imwrite(img_dst, crop_img)"],"metadata":{"id":"vh6V_BqCvjHv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["파일명을 참고하여 이미지를 open 혹은 close 폴더로 옮긴다.\n","\n","\n","\n","```python\n","# /content/drive/MyDrive/CAB/CAB_dataset/fs/\n","# ├── open/\n","# │   └── 파일명_클래스 이름_Open.jpg\n","# ├── close/\n","#      └── 파일명_클래스 이름_Close.jpg\n","```"],"metadata":{"id":"bvDgDHKJ3zKG"}},{"cell_type":"code","source":["data_root = \"/content/drive/MyDrive/CAB/CAB_dataset/\"\n","\n","fs_root = os.path.join(data_root, \"fs\")\n","open_root = os.path.join(fs_root, \"open\")\n","close_root = os.path.join(fs_root, \"close\")\n","\n","os.makedirs(open_root, exist_ok=False)\n","os.makedirs(close_root, exist_ok=False)"],"metadata":{"id":"LUZfhCEi3_bO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for file in os.listdir(eyes_mouth):\n","  if file.endswith('.jpg') and 'Open' in file:\n","    img_src = os.path.join(eyes_mouth, file)\n","    img_dst = os.path.join(open_root, file)\n","\n","  elif file.endswith('.jpg') and 'Close' in file:\n","    img_src = os.path.join(eyes_mouth, file)\n","    img_dst = os.path.join(close_root, file)\n","\n","  shutil.copy2(img_src, img_dst)\n"],"metadata":{"id":"oZyD0U5H4VmH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**데이터 undersampling**"],"metadata":{"id":"ZKkNWBujF9fr"}},{"cell_type":"markdown","source":["다음으로, 모든 Image의 절대 경로가 적힌 리스트를 만든다."],"metadata":{"id":"F4WkYpwj8OIQ"}},{"cell_type":"code","source":["transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5), (0.5))\n","])\n","\n","dataset = ImageFolder(root=fs_root, transform=transform)\n","\n","class_to_idx = dataset.class_to_idx\n","print(f'클래스별 인덱스: {class_to_idx}')\n","\n","class_counts = [0] * len(class_to_idx)\n","for _, target in dataset.samples:\n","    class_counts[target] += 1\n","\n","print(f\"클래스별 샘플 수: {class_counts}\")\n","print(f\"데이터셋 크기: {len(dataset)}\")\n"],"metadata":{"id":"bqOahdk_F7A5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731941533380,"user_tz":-540,"elapsed":75122,"user":{"displayName":"정서영","userId":"11814773768061491291"}},"outputId":"22dd9ee4-f087-4133-e6b2-8b5da437ae14"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["클래스별 인덱스: {'close': 0, 'open': 1}\n","클래스별 샘플 수: [8661, 15364]\n","데이터셋 크기: 24025\n"]}]},{"cell_type":"markdown","source":["데이터 불균형 문제와 제공된 GPU RAM, 학습 속도 등을 고려하여 클래스별 샘플 수를 줄여준다.\n","\n","여기서는 실험 결과 클래스별 샘플 수를 2500개로 하는 것이 이상적이었다."],"metadata":{"id":"8F-b8_vqAK2H"}},{"cell_type":"code","source":["close_samples = [(sample, target) for sample, target in dataset.samples if target == 0]\n","open_samples = [(sample, target) for sample, target in dataset.samples if target == 1]\n","\n","open_samples = open_samples[:2500]\n","close_samples = close_samples[:2500]\n","\n","dataset.samples = close_samples + open_samples"],"metadata":{"id":"tCdtlnJQAOvK","executionInfo":{"status":"ok","timestamp":1731941533381,"user_tz":-540,"elapsed":6,"user":{"displayName":"정서영","userId":"11814773768061491291"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["**train/val/test loader 생성**"],"metadata":{"id":"N-r0VJK1rG6d"}},{"cell_type":"markdown","source":["전체 이미지를 6:2:2 비율로 나누어 train set, validation set, test set을 생성한다.\n","\n","ImageFolder는 instance를 직접 섞는 걸 허용하지 않는다. 따라서 무작위로 섞인 인덱스를 이용하여 훈련/검증/테스트 데이터셋을 만든다.\n"],"metadata":{"id":"sd-HtlAr84WI"}},{"cell_type":"code","source":["dataset_indices = list(range(len(dataset)))\n","\n","random.shuffle(dataset_indices)\n","\n","train_size = int(0.6 * len(dataset))\n","val_size = int(0.2 * len(dataset))\n","test_size = val_size\n","\n","train_dataset = Subset(dataset, dataset_indices[:train_size])\n","val_dataset = Subset(dataset, dataset_indices[train_size:train_size+val_size])\n","test_dataset = Subset(dataset, dataset_indices[train_size+val_size:])"],"metadata":{"id":"1-Q8MSWf7Ywi","executionInfo":{"status":"ok","timestamp":1731941533381,"user_tz":-540,"elapsed":5,"user":{"displayName":"정서영","userId":"11814773768061491291"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["print(\"length of train dataset: \", len(train_dataset))\n","print(\"length of val dataset: \", len(val_dataset))\n","print(\"length of test dataset: \", len(test_dataset))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QUoDyBso87dh","executionInfo":{"status":"ok","timestamp":1731941533381,"user_tz":-540,"elapsed":5,"user":{"displayName":"정서영","userId":"11814773768061491291"}},"outputId":"7920421e-5a40-46d5-9f07-60af424b60ce"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["length of train dataset:  3000\n","length of val dataset:  1000\n","length of test dataset:  1000\n"]}]},{"cell_type":"markdown","source":["다음으로, 훈련/검증/테스트 데이터로더를 생성한다.\n","\n","- num_workers: 2개의 프로세서가 병렬로 데이터를 불러와 이력 데이터가 더 빨리 준비될 수 있도록 한다.\n","- pin_memory=True: CPU에서 GPU로 데이터를 전송할 때 발생하는 복사 작업을 빠르게 할 수 있도록 도와준다."],"metadata":{"id":"bfDBuE8X9K8N"}},{"cell_type":"code","source":["# 데이터 로더 생성\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,  num_workers=2, pin_memory=True)\n","val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True,  num_workers=2, pin_memory=True)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True, num_workers=2, pin_memory=True)"],"metadata":{"id":"UFQ3W3NwFg7k","executionInfo":{"status":"ok","timestamp":1731941533381,"user_tz":-540,"elapsed":4,"user":{"displayName":"정서영","userId":"11814773768061491291"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["**분류 모델 생성**"],"metadata":{"id":"Glrb9tv0FTuh"}},{"cell_type":"markdown","source":["분류 모델로는 이전 프로젝트에서 우수한 성능을 보였던 ResNet50 모델을 선택하였다. 최적화 기법으로는 Adam optimizer를 사용하였으며, 학습률(lr)은 0.0001로 설정하였다. 또한, 배치 사이즈(batch size)는 32로 초기화하였다.\n","\n","(지난 프로젝트 결과, ResNet50 모델에서 <Adam optimizer/lr = 0.0001/batch size = 64> 조합일 때 가장 최상의 결과를 얻었다. 이번 프로젝트에서도 최상의 결과를 얻고자 비슷한 조합을 사용했으며, 제공된 GPU RAM을 고려하여 batch size만 32로 수정하였다.)"],"metadata":{"id":"UpOtc48YpnQ3"}},{"cell_type":"code","source":["resnet50 = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)"],"metadata":{"id":"-lBReLhf-SQH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731941663676,"user_tz":-540,"elapsed":2067,"user":{"displayName":"정서영","userId":"11814773768061491291"}},"outputId":"b7576a4b-ea9f-4918-9131-6641f877d83b"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n","100%|██████████| 97.8M/97.8M [00:00<00:00, 189MB/s]\n"]}]},{"cell_type":"code","source":["class MyResNet50(nn.Module):\n","    def __init__(self, pretrained_model):\n","        super(MyResNet50, self).__init__()\n","        self.backbone = pretrained_model\n","\n","        self.dropout = nn.Dropout(0.3)\n","        self.extra_layer = nn.Linear(1000, 2) # open/close 판단\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, x):\n","        x = self.backbone(x)\n","        x = self.dropout(x)\n","        x = self.softmax(self.extra_layer(x))\n","        return x"],"metadata":{"id":"ScoNb4kk-iiZ","executionInfo":{"status":"ok","timestamp":1731941665683,"user_tz":-540,"elapsed":4,"user":{"displayName":"정서영","userId":"11814773768061491291"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["myresnet50 = MyResNet50(resnet50)\n","\n","optimizer = optim.Adam(myresnet50.parameters(), lr=0.0001)\n","criterion = nn.CrossEntropyLoss()\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"i6cW-ceX-wFd","executionInfo":{"status":"ok","timestamp":1731941667649,"user_tz":-540,"elapsed":3,"user":{"displayName":"정서영","userId":"11814773768061491291"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["**훈련/검증/테스트 함수 정의**"],"metadata":{"id":"3AsVQaiUFNpi"}},{"cell_type":"code","source":["def train(model):\n","  model.train()\n","\n","  for batch_idx, (data, target) in enumerate(train_loader):\n","    data, target = data.to(device), target.to(device)\n","\n","    optimizer.zero_grad()\n","\n","    output = model(data)\n","\n","    loss = criterion(output, target)\n","    loss.backward()\n","    optimizer.step()"],"metadata":{"id":"QksJsWa3HADz","executionInfo":{"status":"ok","timestamp":1731941669921,"user_tz":-540,"elapsed":3,"user":{"displayName":"정서영","userId":"11814773768061491291"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["def val(model, epoch):\n","  model.eval()\n","\n","  val_loss = 0\n","  correct = 0\n","\n","  with torch.no_grad():\n","    for data, target in val_loader:\n","      data, target = data.to(device), target.to(device)\n","\n","      output = model(data)\n","\n","      val_loss += criterion(output, target).item()\n","      pred = output.argmax(dim=1, keepdim=True)\n","      correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","  val_loss /= len(val_loader.dataset)\n","  accuracy = 100. * correct / len(val_loader.dataset)\n","\n","  print(f\"Epoch: {epoch}, Average loss: {val_loss:.4f}, Accuracy: {correct}/{len(val_loader.dataset)} ({accuracy:.2f}%)\")\n","\n","  return val_loss"],"metadata":{"id":"EDf95mlmqHaF","executionInfo":{"status":"ok","timestamp":1731941670390,"user_tz":-540,"elapsed":2,"user":{"displayName":"정서영","userId":"11814773768061491291"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["def test(model):\n","  model.eval()\n","\n","  test_loss = 0\n","  correct = 0\n","\n","  with torch.no_grad():\n","    for data, target in test_loader:\n","      data, target = data.to(device), target.to(device)\n","\n","      output = model(data)\n","\n","      test_loss += criterion(output, target).item()\n","      pred = output.argmax(dim=1, keepdim=True)\n","      correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","  test_loss /= len(test_loader.dataset)\n","  accuracy = 100. * correct / len(test_loader.dataset)\n","\n","  print(f\"Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)\")\n"],"metadata":{"id":"n3Ixf3LoqK98","executionInfo":{"status":"ok","timestamp":1731941673056,"user_tz":-540,"elapsed":2,"user":{"displayName":"정서영","userId":"11814773768061491291"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["**EarlyStopping 정의**"],"metadata":{"id":"JTFcBL6jrOdy"}},{"cell_type":"markdown","source":["overfitting을 방지하기 위해 earlystopping을 사용한다."],"metadata":{"id":"X4bLt8dmFSDW"}},{"cell_type":"code","source":["class EarlyStopping:\n","    def __init__(self, patience=10, verbose=False, counter=0, best_loss=float('inf')):\n","        self.patience = patience\n","        self.verbose = verbose\n","        self.counter = counter\n","        self.best_loss = best_loss\n","        self.early_stop = False\n","\n","    def __call__(self, val_loss, model):\n","        if val_loss < self.best_loss:\n","            self.counter = 0\n","            self.best_loss = val_loss\n","            torch.save(model.state_dict(), '/content/drive/MyDrive/CAB/CAB_dataset/model/best_resnet_model.pth')\n","        else:\n","            self.counter += 1\n","            if self.counter >= self.patience:\n","              self.early_stop = True"],"metadata":{"id":"tWzOvaWMHGqB","executionInfo":{"status":"ok","timestamp":1731941673544,"user_tz":-540,"elapsed":2,"user":{"displayName":"정서영","userId":"11814773768061491291"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["num_epochs = 50"],"metadata":{"id":"ugVjepZCGLPD","executionInfo":{"status":"ok","timestamp":1731941675564,"user_tz":-540,"elapsed":3,"user":{"displayName":"정서영","userId":"11814773768061491291"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["myresnet50.to(device)\n","\n","early_stopping = EarlyStopping(verbose=True)\n","\n","for epoch in range(1, num_epochs + 1):\n","  train(myresnet50)\n","  val_loss = val(myresnet50, epoch)\n","\n","  early_stopping(val_loss, myresnet50)\n","\n","  if early_stopping.early_stop:\n","    print(\"************************************************************\\nEarly stop!\")\n","    myresnet50.load_state_dict(torch.load('/content/drive/MyDrive/CAB/CAB_dataset/model/best_resnet_model.pth', map_location=device))\n","    test(myresnet50)\n","    break"],"metadata":{"id":"wXsW0jarHI_Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731944954240,"user_tz":-540,"elapsed":3268852,"user":{"displayName":"정서영","userId":"11814773768061491291"}},"outputId":"d019ba50-3708-425a-e3f2-b2c16b1e993f"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1, Average loss: 0.0127, Accuracy: 913/1000 (91.30%)\n","Epoch: 2, Average loss: 0.0123, Accuracy: 931/1000 (93.10%)\n","Epoch: 3, Average loss: 0.0111, Accuracy: 965/1000 (96.50%)\n","Epoch: 4, Average loss: 0.0115, Accuracy: 949/1000 (94.90%)\n","Epoch: 5, Average loss: 0.0113, Accuracy: 957/1000 (95.70%)\n","Epoch: 6, Average loss: 0.0112, Accuracy: 962/1000 (96.20%)\n","Epoch: 7, Average loss: 0.0112, Accuracy: 961/1000 (96.10%)\n","Epoch: 8, Average loss: 0.0115, Accuracy: 948/1000 (94.80%)\n","Epoch: 9, Average loss: 0.0111, Accuracy: 967/1000 (96.70%)\n","Epoch: 10, Average loss: 0.0116, Accuracy: 948/1000 (94.80%)\n","Epoch: 11, Average loss: 0.0109, Accuracy: 974/1000 (97.40%)\n","Epoch: 12, Average loss: 0.0112, Accuracy: 960/1000 (96.00%)\n","Epoch: 13, Average loss: 0.0111, Accuracy: 966/1000 (96.60%)\n","Epoch: 14, Average loss: 0.0111, Accuracy: 968/1000 (96.80%)\n","Epoch: 15, Average loss: 0.0109, Accuracy: 972/1000 (97.20%)\n","Epoch: 16, Average loss: 0.0109, Accuracy: 972/1000 (97.20%)\n","Epoch: 17, Average loss: 0.0111, Accuracy: 966/1000 (96.60%)\n","Epoch: 18, Average loss: 0.0112, Accuracy: 957/1000 (95.70%)\n","Epoch: 19, Average loss: 0.0108, Accuracy: 977/1000 (97.70%)\n","Epoch: 20, Average loss: 0.0117, Accuracy: 948/1000 (94.80%)\n","Epoch: 21, Average loss: 0.0111, Accuracy: 967/1000 (96.70%)\n","Epoch: 22, Average loss: 0.0109, Accuracy: 973/1000 (97.30%)\n","Epoch: 23, Average loss: 0.0110, Accuracy: 971/1000 (97.10%)\n","Epoch: 24, Average loss: 0.0109, Accuracy: 972/1000 (97.20%)\n","Epoch: 25, Average loss: 0.0108, Accuracy: 975/1000 (97.50%)\n","Epoch: 26, Average loss: 0.0107, Accuracy: 977/1000 (97.70%)\n","Epoch: 27, Average loss: 0.0109, Accuracy: 974/1000 (97.40%)\n","Epoch: 28, Average loss: 0.0114, Accuracy: 963/1000 (96.30%)\n","Epoch: 29, Average loss: 0.0108, Accuracy: 974/1000 (97.40%)\n","Epoch: 30, Average loss: 0.0116, Accuracy: 950/1000 (95.00%)\n","Epoch: 31, Average loss: 0.0110, Accuracy: 965/1000 (96.50%)\n","Epoch: 32, Average loss: 0.0109, Accuracy: 972/1000 (97.20%)\n","Epoch: 33, Average loss: 0.0109, Accuracy: 972/1000 (97.20%)\n","Epoch: 34, Average loss: 0.0108, Accuracy: 974/1000 (97.40%)\n","Epoch: 35, Average loss: 0.0112, Accuracy: 960/1000 (96.00%)\n","Epoch: 36, Average loss: 0.0109, Accuracy: 972/1000 (97.20%)\n","************************************************************\n","Early stop!\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-17-f6b120e84d28>:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  myresnet50.load_state_dict(torch.load('/content/drive/MyDrive/CAB/CAB_dataset/model/best_resnet_model.pth', map_location=device))\n"]},{"output_type":"stream","name":"stdout","text":["Average loss: 0.0106, Accuracy: 980/1000 (98.00%)\n"]}]}]}